import os
import json
import requests
from bs4 import BeautifulSoup

# iThome è³‡å®‰æ–°èé é¢ URL
ITHOME_URL = "https://www.ithome.com.tw/tags/è³‡å®‰"
NEWS_FILE = "data/news.json"  # å­˜å„²æ–°èçš„ JSON æª”æ¡ˆ

def fetch_news_content(url):
    """å¾æ–°èé€£çµæŠ“å–å®Œæ•´æ–‡ç« å…§å®¹"""
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()  # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        soup = BeautifulSoup(response.text, "html.parser")

        content_div = soup.find("div", class_="field-items")
        if content_div:
            paragraphs = content_div.find_all("p")
            content = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())
            print(f"ğŸ” æŠ“å–å…§å®¹é•·åº¦: {len(content)} å­—")
            return content

        print(f"âš ï¸ æœªèƒ½æŠ“å–å®Œæ•´å…§å®¹: {url}")
        return "âŒ ç„¡æ³•ç²å–å®Œæ•´æ–°èå…§å®¹"

    except Exception as e:
        print(f"âš ï¸ æŠ“å–æ–°èå…§å®¹éŒ¯èª¤: {e}")
        return "âŒ ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦"

def fetch_news():
    """æŠ“å– iThome æœ€æ–° 5 ç¯‡è³‡å®‰æ–°è"""
    response = requests.get(ITHOME_URL, headers={"User-Agent": "Mozilla/5.0"})
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    news_items = soup.find_all("div", class_="view-list-item", limit=5)  # åªæŠ“å–æœ€æ–° 5 ç¯‡
    news_list = []

    for item in news_items:
        a_tag = item.find("a")
        if not a_tag:
            continue

        title = a_tag.text.strip()
        link = "https://www.ithome.com.tw" + a_tag["href"]
        content = fetch_news_content(link)  # å–å¾—å®Œæ•´æ–°èå…§å®¹

        news_list.append({"title": title, "link": link, "content": content})
        print(f"âœ… å·²æŠ“å–: {title}")

    os.makedirs(os.path.dirname(NEWS_FILE), exist_ok=True)
    with open(NEWS_FILE, "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=4)

    print(f"âœ… æ‰€æœ‰æ–°èå·²å„²å­˜è‡³ {NEWS_FILE}")

if __name__ == "__main__":
    fetch_news()